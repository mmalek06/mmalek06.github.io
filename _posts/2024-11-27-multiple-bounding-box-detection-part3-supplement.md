---
layout: post
title: "Multiple bounding box detection, Part 3 - fine tuning the backbone network"
date: 2025-11-23 00:00:00 -0000
categories: Python
tags: ["python", "pytorch", "transfer learning", "image vision", "math"]
---

# Multiple bounding box detection, Part 3 - fine tuning the backbone network: supplement

In my [previous post](https://mmalek06.github.io/python/2024/11/23/multiple-bounding-box-detection-part3.html) I talked about fine-tuning a backbone network. What I didn't realize was that the dataset didn't contain as many positive class instances as it could. In the [second post](https://mmalek06.github.io/python/2024/11/02/multiple-bounding-box-detection-part2.html) of this series I saved the proposals generated by selective search to the disk. What I should have done in that step, is adding the image fragments that represent the cracks as defined in the coco files definitions. As it turns out, their presence changes the outcome for two of the best models trained in the previous post.

## The code

The code that I had to add is just a single function. It just loads bounding boxes as defined in the coco files, then crops the images accordingly and lastly, resizes them so that they conform to the backbone network requirements. Note the `1_0.1` fragment right before re-appending the file extension. The first part is the IoU threshold which obviously equals 1, the other is the class label - 1 as well.

```python
transforms = v2.Compose([
    v2.ToPILImage(),
    v2.Resize((224, 224))
])


def extract_and_save_bboxes(images_dir: str, annotations_file: str, output_dir: str) -> None:
    with open(annotations_file, 'r') as f:
        coco_data = json.load(f)

    os.makedirs(output_dir, exist_ok=True)

    image_id_to_file = {image['id']: image['file_name'] for image in coco_data['images']}
    processed_count = 0
    image_crop_count = {}

    for annotation in coco_data['annotations']:
        bbox = annotation['bbox']
        image_id = annotation['image_id']

        if not bbox or len(bbox) != 4:
            continue

        image_file = image_id_to_file.get(image_id)

        if not image_file:
            continue

        image_path = os.path.join(images_dir, image_file)
        image = cv2.imread(image_path)

        if image is None:
            print(f"Could not load image: {image_path}")
            continue

        x, y, w, h = map(int, bbox)
        cropped_image = image[y:y+h, x:x+w]
        cropped_image = torch.tensor(cropped_image).permute(2, 0, 1)
        resized_proposal = transforms(cropped_image)
        image_name, ext = os.path.splitext(image_file)

        if image_name not in image_crop_count:
            image_crop_count[image_name] = 0

        counter = image_crop_count[image_name]
        image_crop_count[image_name] += 1
        output_file = os.path.join(output_dir, f"{image_name}.{counter}.1_0.1{ext}")

        resized_proposal.save(output_file)

        processed_count += 1

        if processed_count % 100 == 0:
            print(f"Processed {processed_count} cropped images.")
```

Now as for the training code - there's a little bit of history here. First I tried rerunning every notebook, but that turned out to be an exercise in futility, because the worst performing networks were performing badly even after supplementing the dataset with more positive instances. However, the results changed drastically for the two best networks. 

<b>Side note:</b>: I'm writing this as if I was re-training different architectures, but I was not doing that. The architectures are the same, what changed was everything around them - IoU thresholds, using different sampling strategies, weighing the classes differently and finally switching to another loss function.


